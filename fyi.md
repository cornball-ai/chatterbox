<!-- Generated by fyi::use_fyi_md() on 2026-02-19 -->
<!-- Regenerate with: fyi::use_fyi_md("chatterbox") -->

# fyi: chatterbox

## Exported Functions (chatterbox::)

| Function | Arguments |
|----------|-----------|
| `chatterbox` | device |
| `chatterbox_handler` | device, traced |
| `compile_chatterbox` | model |
| `compute_mel_spectrogram` | y, n_fft, n_mels, sr, hop_size, win_size, fmin, fmax, center |
| `compute_mel_spectrogram_ve` | y, sr |
| `compute_speaker_embedding` | model, audio, sr, overlap |
| `compute_xvector_embedding` | model, audio, sr |
| `create_s3gen_vocoder` | device |
| `create_voice_embedding` | model, audio, sample_rate, autocast |
| `decode_tokens` | tokenizer, ids |
| `download_chatterbox_models` | force |
| `download_chatterbox_turbo_models` | force |
| `generate` | model, text, voice, exaggeration, cfg_weight, temperature, top_p, autocast, traced, backend |
| `get_model_paths` |  |
| `get_turbo_model_paths` |  |
| `load_bpe_tokenizer` | vocab_path |
| `load_campplus_weights` | model, state_dict, prefix |
| `load_chatterbox` | model, compiled |
| `load_hifigan_weights` | model, state_dict, prefix |
| `load_s3gen` | path, device |
| `load_s3gen_weights` | model, state_dict |
| `load_s3tokenizer_weights` | model, state_dict, prefix |
| `models_available` |  |
| `quick_tts` | text, reference_audio, output_path, device, autocast |
| `read_audio` | path |
| `read_safetensors` | path, device |
| `resample_audio` | samples, from_sr, to_sr |
| `s3_tokenizer` | ... |
| `serve_chatterbox` | port, host, device, traced |
| `t3_inference` | model, cond, text_tokens, max_new_tokens, temperature, cfg_weight, top_p, min_p, repetition_penalty |
| `t3_inference_cpp` | model, cond, text_tokens, max_new_tokens, temperature, cfg_weight, top_p, min_p, repetition_penalty, max_cache_len |
| `text_to_tokens` | tokenizer, text, normalize, device |
| `tts_chunked` | model, text, voice, chunk_size, ... |
| `tts_to_file` | model, text, voice, output_path, ... |
| `turbo_models_available` |  |
| `write_audio` | samples, sr, path |


## Internal Functions (chatterbox:::)

| Function | Arguments |
|----------|-----------|
| `.get_cpp_weights` | model |
| `apply_llama3_rope_scaling` | inv_freq, scaling, dim |
| `apply_rotary_emb_s3` | xq, xk, freqs_cis |
| `apply_rotary_pos_emb` | q, k, cos, sin, position_ids |
| `attention_block` | ... |
| `basic_res_block` | ... |
| `basic_transformer_block` | ... |
| `cam_dense_tdnn_block` | ... |
| `cam_dense_tdnn_layer` | ... |
| `cam_layer` | ... |
| `campplus` | ... |
| `causal_block1d` | ... |
| `causal_cfm` | ... |
| `causal_conv1d` | ... |
| `causal_masked_diff_xvec` | ... |
| `causal_resnet_block1d` | ... |
| `cfm_attention` | ... |
| `cfm_estimator` | ... |
| `compile_module_forward` | module, label, ... |
| `compute_rope_frequencies` | dim, max_seq_len, theta, scaling, device |
| `compute_ve_mel` | wav, config |
| `conformer_encoder_layer` | ... |
| `conv_rnn_f0_predictor` | ... |
| `create_kv_cache` | batch_size, n_layers, n_heads, head_dim, max_len, device |
| `create_mel_filterbank` | sr, n_fft, n_mels, fmin, fmax, norm, htk |
| `dense_layer` | ... |
| `drop_invalid_tokens` | tokens |
| `espnet_rel_positional_encoding` | ... |
| `fcm_module` | ... |
| `feed_forward` | ... |
| `fsmn_multi_head_attention` | ... |
| `fsq_codebook` | ... |
| `fsq_vector_quantization` | ... |
| `gelu_with_proj` | ... |
| `get_conv_padding` | kernel_size, dilation |
| `get_sdpa` |  |
| `get_traced_layers` | model, max_cache_len |
| `hifigan_resblock` | ... |
| `hift_generator` | ... |
| `init_cache_from_first` | cache, past_key_values |
| `is_loaded` | model |
| `learned_position_embeddings` | ... |
| `linear_no_subsampling` | ... |
| `llama_attention` | ... |
| `llama_config_520m` |  |
| `llama_decoder_layer` | ... |
| `llama_mlp` | ... |
| `llama_model` | ... |
| `llama_rms_norm` | ... |
| `load_cfm_estimator_weights` | estimator, state_dict, prefix |
| `load_conformer_encoder_weights` | model, state_dict, prefix |
| `load_llama_weights` | model, state_dict, prefix |
| `load_t3_weights` | model, state_dict |
| `load_tokenizer` | vocab_path |
| `load_voice_encoder_weights` | model, state_dict |
| `make_non_pad_mask_s3` | lengths, max_len |
| `make_pad_mask` | lengths, max_len |
| `mask_to_bias` | mask, dtype |
| `mish_activation` | ... |
| `pad_audio_for_tokenizer` | wav, sr |
| `perceiver_resampler` | ... |
| `positionwise_feedforward` | ... |
| `pre_lookahead_layer` | ... |
| `precompute_freqs_cis` | dim, end, theta |
| `print.chatterbox` | x, ... |
| `print.voice_embedding` | x, ... |
| `punc_norm` | text |
| `reflection_pad1d` | ... |
| `rel_position_attention` | ... |
| `rotate_half` | x |
| `s3_audio_encoder` | ... |
| `s3_log_mel_spectrogram` | audio, mel_filters, window, n_fft, device |
| `s3_multi_head_attention` | ... |
| `s3_residual_attention_block` | ... |
| `s3_tokenizer_config` | n_mels, n_audio_state, n_audio_head, n_audio_layer, n_codebook_size |
| `s3gen` | ... |
| `sine_gen` | ... |
| `sinusoidal_pos_emb` | ... |
| `snake_activation` | ... |
| `source_module_hn_nsf` | ... |
| `statistics_pooling` | x |
| `t3_cond` | speaker_emb, cond_prompt_speech_tokens, cond_prompt_speech_emb, emotion_adv |
| `t3_cond_enc` | ... |
| `t3_cond_to_device` | cond, device |
| `t3_config_english` |  |
| `t3_inference_traced` | model, cond, text_tokens, max_new_tokens, temperature, cfg_weight, top_p, min_p, repetition_penalty, max_cache_len |
| `t3_model` | ... |
| `tdnn_layer` | ... |
| `timestep_embedding` | ... |
| `tokenize_text` | tokenizer, text |
| `traceable_attention` | ... |
| `traceable_decoder_layer` | ... |
| `traceable_kv_projector` | ... |
| `traceable_transformer_cached` | ... |
| `traceable_transformer_first` | ... |
| `transit_layer` | ... |
| `transpose_layer` | ... |
| `update_kv_cache` | cache, layer_idx, new_k, new_v, position |
| `update_valid_mask` | cache, position |
| `upsample_1d` | ... |
| `upsample_conformer_encoder` | input_size, output_size, num_blocks |
| `upsample_conformer_encoder_full` | ... |
| `voice_encoder` | ... |
| `voice_encoder_config` |  |


## Options (chatterbox)

| Option | File | Type |
|--------|------|------|
| `chatterbox.consent` | download.R | get |
| `chatterbox.consent` | download.R | set |


# Documentation: chatterbox

## apply_llama3_rope_scaling

### Apply Llama3-style RoPE scaling

#### Description

Apply Llama3-style RoPE scaling

#### Usage

```r
apply_llama3_rope_scaling(inv_freq, scaling, dim)
```

#### Arguments

- **`inv_freq`**: Inverse frequencies
- **`scaling`**: Scaling configuration
- **`dim`**: Dimension

#### Value

Scaled inverse frequencies


## apply_rotary_emb_s3

### Apply rotary position embeddings

#### Description

Apply rotary position embeddings

#### Usage

```r
apply_rotary_emb_s3(xq, xk, freqs_cis)
```

#### Arguments

- **`xq`**: Query tensor
- **`xk`**: Key tensor
- **`freqs_cis`**: Precomputed frequencies

#### Value

List with rotated q and k


## apply_rotary_pos_emb

### Apply rotary position embeddings to Q and K

#### Description

Apply rotary position embeddings to Q and K

#### Usage

```r
apply_rotary_pos_emb(q, k, cos, sin, position_ids)
```

#### Arguments

- **`q`**: Query tensor (batch, heads, seq, head_dim)
- **`k`**: Key tensor (batch, heads, seq, head_dim)
- **`cos`**: Cosine cache
- **`sin`**: Sine cache
- **`position_ids`**: Position indices

#### Value

List with rotated q and k


## attention_block

### Attention block for perceiver

#### Description

Attention block for perceiver

#### Usage

```r
attention_block
```


## basic_res_block

### Basic residual block for FCM

#### Description

Basic residual block for FCM

#### Usage

```r
basic_res_block
```


## basic_transformer_block

### Basic transformer block

#### Description

Basic transformer block

#### Usage

```r
basic_transformer_block
```


## cam_dense_tdnn_block

### CAM Dense TDNN Block (multiple layers with dense connections)

#### Description

CAM Dense TDNN Block (multiple layers with dense connections)

#### Usage

```r
cam_dense_tdnn_block
```


## cam_dense_tdnn_layer

### CAM Dense TDNN Layer

#### Description

CAM Dense TDNN Layer

#### Usage

```r
cam_dense_tdnn_layer
```


## cam_layer

### CAM (Context-Aware Masking) Layer

#### Description

CAM (Context-Aware Masking) Layer

#### Usage

```r
cam_layer
```


## campplus

### CAMPPlus speaker encoder

#### Description

CAMPPlus speaker encoder

#### Usage

```r
campplus
```


## causal_block1d

### Causal Block 1D - CausalConv + LayerNorm + Mish

#### Description

Causal Block 1D - CausalConv + LayerNorm + Mish

#### Usage

```r
causal_block1d
```


## causal_cfm

### Causal Conditional Flow Matching

#### Description

Causal Conditional Flow Matching

#### Usage

```r
causal_cfm(in_channels = 320, out_channels = 80, spk_emb_dim = 80)
```

#### Arguments

- **`in_channels`**: Input channels (x + mu + spks + cond)
- **`out_channels`**: Output channels (mel bins)
- **`spk_emb_dim`**: Speaker embedding dimension

#### Value

nn_module


## causal_conv1d

### Causal Conv1d - pads left only

#### Description

Causal Conv1d - pads left only

#### Usage

```r
causal_conv1d
```


## causal_masked_diff_xvec

### Causal Masked Diff with Xvector

#### Description

Causal Masked Diff with Xvector

#### Usage

```r
causal_masked_diff_xvec
```


## causal_resnet_block1d

### Causal ResNet Block 1D

#### Description

Causal ResNet Block 1D

#### Usage

```r
causal_resnet_block1d
```


## cfm_attention

### Self-attention for transformer block

#### Description

Self-attention for transformer block

#### Usage

```r
cfm_attention
```


## cfm_estimator

### CFM Estimator (ConditionalDecoder)

#### Description

UNet-style architecture with:
- 1 down block (320 -> 256, with 4 transformer blocks)
- 12 mid blocks (256 -> 256, each with 4 transformer blocks)
- 1 up block (512 -> 256 with skip connection, 4 transformer blocks)

#### Usage

```r
cfm_estimator
```


## chatterbox_handler

### Create Chatterbox HTTP Request Handler

#### Description

Returns a handler function suitable for `gpu.ctl::serve()`.
The model is loaded once at handler creation and reused for all requests.

#### Usage

```r
chatterbox_handler(device = "cuda", traced = TRUE)
```

#### Arguments

- **`device`**: Device to use ("cpu" or "cuda"). Default "cuda".
- **`traced`**: Use JIT-traced inference for faster generation. Default TRUE.

#### Value

A function with signature
  `function(method, path, headers, body)` for use with
  `gpu.ctl::serve()`.

#### Examples

```r
handler <- chatterbox_handler()
gpu.ctl::serve(7810, handler)
```


## CHATTERBOX_REPO

### Model Download Utilities

#### Description

Download Chatterbox models from HuggingFace using hfhub.
Requires explicit download with user consent (no auto-download).

#### Usage

```r
CHATTERBOX_REPO
```


## chatterbox

### Create Chatterbox TTS model

#### Description

Create Chatterbox TTS model

#### Usage

```r
chatterbox(device = "cpu")
```

#### Arguments

- **`device`**: Device to use ("cpu", "cuda", "mps", etc.)

#### Value

Chatterbox TTS model object


## compile_chatterbox

### Compile chatterbox model sub-modules

#### Description

Walks the loaded model and compiles eligible sub-modules using
Rtorch's torchlang compiler. Compiled modules fuse elementwise
operation chains into single kernel calls.

#### Usage

```r
compile_chatterbox(model)
```

#### Arguments

- **`model`**: A loaded chatterbox model

#### Value

The model with compiled sub-modules


## compile_module_forward

### Compile a sub-module's forward method

#### Description

Traces the module with Rtorch::compile() and patches its forward
method to use the compiled path. Returns the module unchanged if
graph breaks are detected.

#### Usage

```r
compile_module_forward(module, label = NULL, ...)
```

#### Arguments

- **`module`**: An nn_module
- **`label`**: Human-readable name for messages
- **`...`**: Named example tensors matching forward() signature

#### Value

The module (forward patched in place if compilation succeeded)


## compute_mel_spectrogram_ve

### Compute mel spectrogram for voice encoder (40 bins, 16kHz)

#### Description

Compute mel spectrogram for voice encoder (40 bins, 16kHz)

#### Usage

```r
compute_mel_spectrogram_ve(y, sr = 16000)
```

#### Arguments

- **`y`**: Audio samples
- **`sr`**: Sample rate (should be 16000)

#### Value

Mel spectrogram (batch, time, 40)


## compute_mel_spectrogram

### Compute mel spectrogram (S3Gen compatible)

#### Description

Compute mel spectrogram (S3Gen compatible)

#### Usage

```r
compute_mel_spectrogram(
  y,
  n_fft = 1920,
  n_mels = 80,
  sr = 24000,
  hop_size = 480,
  win_size = 1920,
  fmin = 0,
  fmax = 8000,
  center = FALSE
)
```

#### Arguments

- **`y`**: Audio samples as torch tensor or numeric vector
- **`n_fft`**: FFT size (default 1920 for 24kHz)
- **`n_mels`**: Number of mel bins (default 80)
- **`sr`**: Sample rate (default 24000)
- **`hop_size`**: Hop size (default 480)
- **`win_size`**: Window size (default 1920)
- **`fmin`**: Minimum frequency (default 0)
- **`fmax`**: Maximum frequency (default 8000)
- **`center`**: Whether to center frames (default FALSE)

#### Value

Mel spectrogram tensor (batch, n_mels, time)


## compute_rope_frequencies

### Compute rotary position embeddings frequencies

#### Description

Compute rotary position embeddings frequencies

#### Usage

```r
compute_rope_frequencies(
  dim,
  max_seq_len,
  theta = 5e+05,
  scaling = NULL,
  device = "cpu"
)
```

#### Arguments

- **`dim`**: Dimension of embeddings
- **`max_seq_len`**: Maximum sequence length
- **`theta`**: Base frequency
- **`scaling`**: Rope scaling configuration (optional)
- **`device`**: Device to create tensors on

#### Value

List with cos and sin caches


## compute_speaker_embedding

### Compute speaker embedding from audio

#### Description

Compute speaker embedding from audio

#### Usage

```r
compute_speaker_embedding(model, audio, sr, overlap = 0.5)
```

#### Arguments

- **`model`**: Voice encoder model
- **`audio`**: Audio samples (numeric vector or tensor)
- **`sr`**: Sample rate of audio
- **`overlap`**: Overlap between partials (default 0.5)

#### Value

Speaker embedding tensor (1, 256)


## compute_ve_mel

### Compute mel spectrogram for voice encoder

#### Description

Uses power spectrum (magnitude^2) without log compression,
matching Python's mel_type="amp" and mel_power=2.0.

#### Usage

```r
compute_ve_mel(wav, config = voice_encoder_config())
```

#### Arguments

- **`wav`**: Audio samples (numeric vector)
- **`config`**: Voice encoder config

#### Value

Mel spectrogram (batch, time, n_mels)


## compute_xvector_embedding

### Compute xvector speaker embedding from audio

#### Description

Compute xvector speaker embedding from audio

#### Usage

```r
compute_xvector_embedding(model, audio, sr)
```

#### Arguments

- **`model`**: CAMPPlus model
- **`audio`**: Audio samples (tensor or numeric)
- **`sr`**: Sample rate

#### Value

Speaker embedding (1, 192)


## conformer_encoder_layer

### Conformer Encoder Layer

#### Description

Single conformer block with attention and feed-forward (no convolution).

#### Usage

```r
conformer_encoder_layer
```


## conv_rnn_f0_predictor

### Convolutional RNN F0 Predictor

#### Description

Convolutional RNN F0 Predictor

#### Usage

```r
conv_rnn_f0_predictor
```


## create_kv_cache

### Create pre-allocated KV cache

#### Description

Create pre-allocated KV cache

#### Usage

```r
create_kv_cache(batch_size, n_layers, n_heads, head_dim, max_len, device)
```

#### Arguments

- **`batch_size`**: Batch size
- **`n_layers`**: Number of transformer layers
- **`n_heads`**: Number of attention heads
- **`head_dim`**: Head dimension
- **`max_len`**: Maximum sequence length
- **`device`**: Device to allocate on

#### Value

List with k_cache, v_cache, valid_mask


## create_mel_filterbank

### Create mel filterbank

#### Description

Create mel filterbank

#### Usage

```r
create_mel_filterbank(
  sr,
  n_fft,
  n_mels,
  fmin = 0,
  fmax = NULL,
  norm = "slaney",
  htk = FALSE
)
```

#### Arguments

- **`sr`**: Sample rate
- **`n_fft`**: FFT size
- **`n_mels`**: Number of mel bins
- **`fmin`**: Minimum frequency
- **`fmax`**: Maximum frequency

#### Value

Mel filterbank matrix (n_mels x (n_fft/2 + 1))


## create_s3gen_vocoder

### Create HiFiGAN vocoder with S3Gen configuration

#### Description

Create HiFiGAN vocoder with S3Gen configuration

#### Usage

```r
create_s3gen_vocoder(device = "cpu")
```

#### Arguments

- **`device`**: Target device

#### Value

HiFTGenerator module


## create_voice_embedding

### Create voice embedding from reference audio

#### Description

Create voice embedding from reference audio

#### Usage

```r
create_voice_embedding(model, audio, sample_rate = NULL, autocast = NULL)
```

#### Arguments

- **`model`**: Chatterbox model
- **`audio`**: Reference audio (file path, numeric vector, or torch tensor)
- **`sample_rate`**: Sample rate of audio (if not a file)
- **`autocast`**: Ignored (kept for API compatibility)

#### Value

Voice embedding that can be used for synthesis


## decode_tokens

### Decode token IDs to text

#### Description

Decode token IDs to text

#### Usage

```r
decode_tokens(tokenizer, ids)
```

#### Arguments

- **`tokenizer`**: Tokenizer object
- **`ids`**: Integer vector or tensor of token IDs

#### Value

Decoded text string


## dense_layer

### Dense layer for final embedding

#### Description

Dense layer for final embedding

#### Usage

```r
dense_layer
```


## dot-get_cpp_weights

### Extract model weights for C++ decode loop

#### Description

Extracts all layer weights into a flat structure suitable for passing
to the C++ decode function. Just creates list references (no copies).

#### Usage

```r
.get_cpp_weights(model)
```

#### Arguments

- **`model`**: T3 model

#### Value

List with layers, final_norm, speech_head, speech_emb, speech_pos_emb


## download_chatterbox_models

### Download Chatterbox Models from HuggingFace

#### Description

Download all Chatterbox model files from HuggingFace.
In interactive sessions, asks for user consent before downloading.

#### Usage

```r
download_chatterbox_models(force = FALSE)
```

#### Arguments

- **`force`**: Re-download even if files exist

#### Value

Named list of local file paths (invisibly)

#### Examples

```r
# Download models (~2GB)
download_chatterbox_models()
```


## download_chatterbox_turbo_models

### Download Chatterbox Turbo Models from HuggingFace

#### Description

Download all Chatterbox Turbo model files from HuggingFace.
In interactive sessions, asks for user consent before downloading.
The turbo model uses flow matching for faster inference.

#### Usage

```r
download_chatterbox_turbo_models(force = FALSE)
```

#### Arguments

- **`force`**: Re-download even if files exist

#### Value

Named list of local file paths (invisibly)

#### Examples

```r
# Download turbo models (~3.8GB)
download_chatterbox_turbo_models()
```


## drop_invalid_tokens

### Drop invalid speech tokens

#### Description

Drop invalid speech tokens

#### Usage

```r
drop_invalid_tokens(tokens)
```

#### Arguments

- **`tokens`**: Token tensor

#### Value

Filtered tokens


## espnet_rel_positional_encoding

### Sinusoidal positional encoding (Espnet RelPositionalEncoding)

#### Description

Creates sinusoidal positional embeddings for use with relative position
attention. Includes scaling by sqrt(d_model) and adds positional embedding
to the input.

#### Usage

```r
espnet_rel_positional_encoding
```


## fcm_module

### Factorized Convolutional Module (FCM)

#### Description

Factorized Convolutional Module (FCM)

#### Usage

```r
fcm_module
```


## feed_forward

### Feed-forward network for transformer
Matches diffusers FeedForward: net = [GELU(proj), Dropout, Linear]

#### Description

Feed-forward network for transformer
Matches diffusers FeedForward: net = [GELU(proj), Dropout, Linear]

#### Usage

```r
feed_forward
```


## fsmn_multi_head_attention

### FSMN Multi-Head Attention

#### Description

Multi-head attention with Frequency-domain Self-attention Memory Network

#### Usage

```r
fsmn_multi_head_attention
```


## fsq_codebook

### FSQ Codebook module

#### Description

FSQ Codebook module

#### Usage

```r
fsq_codebook
```


## fsq_vector_quantization

### FSQ Vector Quantization wrapper

#### Description

FSQ Vector Quantization wrapper

#### Usage

```r
fsq_vector_quantization
```


## gelu_with_proj

### GELU activation with projection (matches diffusers GELU structure)

#### Description

GELU activation with projection (matches diffusers GELU structure)

#### Usage

```r
gelu_with_proj
```


## generate

### Generate speech from text

#### Description

Generate speech from text

#### Usage

```r
generate(
  model,
  text,
  voice,
  exaggeration = 0.5,
  cfg_weight = 0.5,
  temperature = 0.8,
  top_p = 0.9,
  autocast = NULL,
  traced = FALSE,
  backend = c("r", "cpp")
)
```

#### Arguments

- **`model`**: Chatterbox model
- **`text`**: Text to synthesize
- **`voice`**: Voice embedding from create_voice_embedding() or path to reference audio
- **`exaggeration`**: Emotion/expression exaggeration level (0-1, default 0.5)
- **`cfg_weight`**: Classifier-free guidance weight (higher = more adherence to text, default 0.5)
- **`temperature`**: Sampling temperature (default 0.8)
- **`top_p`**: Top-p (nucleus) sampling threshold (default 0.9)
- **`autocast`**: Use mixed precision (float16) on CUDA for faster inference (default TRUE on CUDA)

#### Value

List with audio (numeric vector) and sample_rate


## get_cache_dir

### Get default cache directory

#### Description

Get default cache directory

#### Usage

```r
get_cache_dir()
```

#### Value

Path to cache directory


## get_conv_padding

### Get padding for convolution

#### Description

Get padding for convolution

#### Usage

```r
get_conv_padding(kernel_size, dilation = 1)
```

#### Arguments

- **`kernel_size`**: Kernel size
- **`dilation`**: Dilation rate

#### Value

Padding size


## get_model_paths

### Get Paths to Downloaded Model Files

#### Description

Get Paths to Downloaded Model Files

#### Usage

```r
get_model_paths()
```

#### Value

Named list of local file paths


## get_traced_layers

### Get or create traced layers for cached inference

#### Description

Get or create traced layers for cached inference

#### Usage

```r
get_traced_layers(model, max_cache_len = 350L)
```

#### Arguments

- **`model`**: T3 model
- **`max_cache_len`**: Maximum cache length

#### Value

List of traced layer modules


## get_turbo_model_paths

### Get Paths to Downloaded Turbo Model Files

#### Description

Get Paths to Downloaded Turbo Model Files

#### Usage

```r
get_turbo_model_paths()
```

#### Value

Named list of local file paths


## hf_download

### Download file from HuggingFace Hub

#### Description

Download file from HuggingFace Hub

#### Usage

```r
hf_download(repo_id, filename, force = FALSE)
```

#### Arguments

- **`repo_id`**: Repository ID (e.g., "ResembleAI/chatterbox")
- **`filename`**: Filename to download
- **`force`**: Re-download even if file exists

#### Value

Local path to downloaded file


## hifigan_resblock

### HiFiGAN Residual Block

#### Description

HiFiGAN Residual Block

#### Usage

```r
hifigan_resblock
```


## hift_generator

### HiFTNet Generator

#### Description

Neural Source Filter + ISTFTNet
Reference: https://arxiv.org/abs/2309.09493

#### Usage

```r
hift_generator
```


## init_cache_from_first

### Initialize cache with first token K/V values

#### Description

Initialize cache with first token K/V values

#### Usage

```r
init_cache_from_first(cache, past_key_values)
```

#### Arguments

- **`cache`**: Cache list from create_kv_cache
- **`past_key_values`**: List of K/V from first forward pass

#### Value

Updated cache (and seq_len as attribute)


## is_loaded

### Check if model is loaded

#### Description

Check if model is loaded

#### Usage

```r
is_loaded(model)
```

#### Arguments

- **`model`**: Chatterbox model

#### Value

TRUE if model is loaded


## learned_position_embeddings

### Learned position embeddings module

#### Description

Learned position embeddings module

#### Usage

```r
learned_position_embeddings
```


## linear_no_subsampling

### Linear No Subsampling layer

#### Description

Projects input to model dimension with layer norm and positional encoding.

#### Usage

```r
linear_no_subsampling
```


## llama_attention

### Llama attention module

#### Description

Llama attention module

#### Usage

```r
llama_attention
```


## llama_config_520m

### Create Llama 520M configuration

#### Description

Create Llama 520M configuration

#### Usage

```r
llama_config_520m()
```

#### Value

List with model configuration


## llama_decoder_layer

### Llama decoder layer

#### Description

Llama decoder layer

#### Usage

```r
llama_decoder_layer
```


## llama_mlp

### Llama MLP module

#### Description

Llama MLP module

#### Usage

```r
llama_mlp
```


## llama_model

### Llama model (decoder only)

#### Description

Llama model (decoder only)

#### Usage

```r
llama_model
```


## llama_rms_norm

### RMS Normalization module

#### Description

RMS Normalization module

#### Usage

```r
llama_rms_norm
```


## load_bpe_tokenizer

### Load tokenizer from JSON file

#### Description

Load tokenizer from JSON file

#### Usage

```r
load_bpe_tokenizer(vocab_path)
```

#### Arguments

- **`vocab_path`**: Path to tokenizer.json

#### Value

Tokenizer object (list)


## load_campplus_weights

### Load CAMPPlus weights from safetensors

#### Description

Load CAMPPlus weights from safetensors

#### Usage

```r
load_campplus_weights(model, state_dict, prefix = "speaker_encoder.")
```

#### Arguments

- **`model`**: CAMPPlus model
- **`state_dict`**: Named list of tensors
- **`prefix`**: Prefix for weight keys (default "speaker_encoder.")

#### Value

Model with loaded weights


## load_cfm_estimator_weights

### Load CFM estimator weights

#### Description

Load CFM estimator weights

#### Usage

```r
load_cfm_estimator_weights(estimator, state_dict, prefix = "")
```

#### Arguments

- **`estimator`**: CFM estimator module
- **`state_dict`**: State dictionary
- **`prefix`**: Key prefix in state dict

#### Value

Number of keys loaded


## load_chatterbox

### Load Chatterbox model weights

#### Description

Load pretrained weights for all model components.
Requires prior download via `download_chatterbox_models`.

#### Usage

```r
load_chatterbox(model, compiled = FALSE)
```

#### Arguments

- **`model`**: Chatterbox model object
- **`compiled`**: If TRUE, compile eligible sub-modules using
`Rtorch::compile()` for fused kernel execution.

#### Value

Chatterbox model with loaded weights


## load_conformer_encoder_weights

### Load Conformer Encoder weights

#### Description

Load Conformer Encoder weights

#### Usage

```r
load_conformer_encoder_weights(model, state_dict, prefix = "flow.encoder.")
```

#### Arguments

- **`model`**: Conformer encoder module
- **`state_dict`**: State dictionary
- **`prefix`**: Key prefix (e.g., "flow.encoder.")


## load_hifigan_weights

### Load HiFiGAN weights from state dictionary

#### Description

Load HiFiGAN weights from state dictionary

#### Usage

```r
load_hifigan_weights(model, state_dict, prefix = "mel2wav.")
```

#### Arguments

- **`model`**: HiFTGenerator model
- **`state_dict`**: Named list of tensors
- **`prefix`**: Prefix for weight keys (default "mel2wav.")

#### Value

Model with loaded weights


## load_llama_weights

### Load weights from safetensors into Llama model

#### Description

Load weights from safetensors into Llama model

#### Usage

```r
load_llama_weights(model, state_dict, prefix = "model.")
```

#### Arguments

- **`model`**: LlamaModel instance
- **`state_dict`**: Named list of tensors from safetensors
- **`prefix`**: Prefix to strip from weight names (default: "model.")

#### Value

Model with loaded weights


## load_s3gen_weights

### Load S3Gen weights

#### Description

Load S3Gen weights

#### Usage

```r
load_s3gen_weights(model, state_dict)
```

#### Arguments

- **`model`**: S3Gen model
- **`state_dict`**: State dictionary from safetensors

#### Value

Model with loaded weights


## load_s3gen

### Load S3Gen from safetensors file

#### Description

Load S3Gen from safetensors file

#### Usage

```r
load_s3gen(path, device = "cpu")
```

#### Arguments

- **`path`**: Path to s3gen.safetensors
- **`device`**: Device to load to ("cpu", "cuda", etc.)

#### Value

S3Gen model with loaded weights


## load_s3tokenizer_weights

### Load S3Tokenizer weights from state dictionary

#### Description

Load S3Tokenizer weights from state dictionary

#### Usage

```r
load_s3tokenizer_weights(model, state_dict, prefix = "tokenizer.")
```

#### Arguments

- **`model`**: S3Tokenizer model
- **`state_dict`**: Named list of tensors
- **`prefix`**: Prefix for weight keys (default "tokenizer.")

#### Value

Model with loaded weights


## load_t3_weights

### Load T3 weights from safetensors

#### Description

Load T3 weights from safetensors

#### Usage

```r
load_t3_weights(model, state_dict)
```

#### Arguments

- **`model`**: T3 model
- **`state_dict`**: Named list of tensors

#### Value

Model with loaded weights


## load_tokenizer

### Load tokenizer from JSON file (internal)

#### Description

Load tokenizer from JSON file (internal)

#### Usage

```r
load_tokenizer(vocab_path)
```

#### Arguments

- **`vocab_path`**: Path to tokenizer.json

#### Value

Tokenizer object (list)


## load_voice_encoder_weights

### Load voice encoder weights from safetensors

#### Description

Load voice encoder weights from safetensors

#### Usage

```r
load_voice_encoder_weights(model, state_dict)
```

#### Arguments

- **`model`**: Voice encoder model
- **`state_dict`**: Named list of tensors

#### Value

Model with loaded weights


## make_non_pad_mask_s3

### Create non-padding mask

#### Description

Create non-padding mask

#### Usage

```r
make_non_pad_mask_s3(lengths, max_len)
```

#### Arguments

- **`lengths`**: Tensor of sequence lengths
- **`max_len`**: Maximum sequence length

#### Value

Boolean mask tensor (TRUE for valid positions)


## make_pad_mask

### Create padding mask

#### Description

Create padding mask

#### Usage

```r
make_pad_mask(lengths, max_len = NULL)
```

#### Arguments

- **`lengths`**: Sequence lengths
- **`max_len`**: Maximum length

#### Value

Boolean mask (TRUE for padded positions)


## mask_to_bias

### Convert mask to attention bias

#### Description

Convert mask to attention bias

#### Usage

```r
mask_to_bias(mask, dtype)
```

#### Arguments

- **`mask`**: Boolean mask
- **`dtype`**: Target dtype

#### Value

Attention bias tensor


## mish_activation

### Mish activation

#### Description

Mish activation

#### Usage

```r
mish_activation
```


## models_available

### Check if Models are Downloaded

#### Description

Check if Models are Downloaded

#### Usage

```r
models_available()
```

#### Value

TRUE if all model files exist locally

#### Examples

```r
models_available()
```


## pad_audio_for_tokenizer

### Pad audio to multiple of token rate

#### Description

Pad audio to multiple of token rate

#### Usage

```r
pad_audio_for_tokenizer(wav, sr)
```

#### Arguments

- **`wav`**: Audio samples
- **`sr`**: Sample rate

#### Value

Padded audio


## perceiver_resampler

### Perceiver resampler for conditioning compression

#### Description

Perceiver resampler for conditioning compression

#### Usage

```r
perceiver_resampler
```


## positionwise_feedforward

### Positionwise Feed Forward

#### Description

Two-layer feed-forward network with SiLU activation.

#### Usage

```r
positionwise_feedforward
```


## pre_lookahead_layer

### Pre-Lookahead Layer

#### Description

Two causal convolutions with residual connection for look-ahead.

#### Usage

```r
pre_lookahead_layer
```


## precompute_freqs_cis

### Precompute rotary position embedding frequencies

#### Description

Precompute rotary position embedding frequencies

#### Usage

```r
precompute_freqs_cis(dim, end, theta = 10000)
```

#### Arguments

- **`dim`**: Dimension (head_dim)
- **`end`**: Maximum sequence length
- **`theta`**: Base frequency

#### Value

Complex frequency tensor


## print.chatterbox

### Print method for chatterbox

#### Description

Print method for chatterbox

#### Usage

```r
printchatterbox(x, ...)
```

#### Arguments

- **`x`**: Chatterbox model
- **`...`**: Ignored


## print.voice_embedding

### Print method for voice_embedding

#### Description

Print method for voice_embedding

#### Usage

```r
printvoice_embedding(x, ...)
```

#### Arguments

- **`x`**: Voice embedding
- **`...`**: Ignored


## punc_norm

### Normalize punctuation for TTS

#### Description

Normalize punctuation for TTS

#### Usage

```r
punc_norm(text)
```

#### Arguments

- **`text`**: Input text

#### Value

Normalized text


## quick_tts

### Quick TTS - one-line text-to-speech

#### Description

Loads model if needed and generates speech. Convenient for quick tests.

#### Usage

```r
quick_tts(
  text,
  reference_audio,
  output_path = NULL,
  device = "cpu",
  autocast = NULL
)
```

#### Arguments

- **`text`**: Text to synthesize
- **`reference_audio`**: Path to reference audio file
- **`output_path`**: Optional output file path. If NULL, returns audio data.
- **`device`**: Device to use
- **`autocast`**: Use mixed precision (float16) on CUDA (default TRUE on CUDA)

#### Value

If output_path is NULL, returns list with audio and sample_rate.
        Otherwise writes to file and returns path invisibly.


## read_audio

### Read audio file

#### Description

Read audio file

#### Usage

```r
read_audio(path)
```

#### Arguments

- **`path`**: Path to audio file (WAV or MP3 format)

#### Value

List with samples (numeric vector normalized to \[-1, 1\]) and sr (sample rate)


## read_safetensors

### Read safetensors file

#### Description

Loads tensors from a safetensors file as Rtorch tensors.

#### Usage

```r
read_safetensors(path, device = "cpu")
```

#### Arguments

- **`path`**: Path to .safetensors file
- **`device`**: Device to load tensors to (ignored, CPU only)

#### Value

Named list of torch tensors


## reflection_pad1d

### Reflection padding for 1D (nn_reflection_pad1d equivalent)

#### Description

Reflection padding for 1D (nn_reflection_pad1d equivalent)

#### Usage

```r
reflection_pad1d
```


## rel_position_attention

### Relative Position Multi-Headed Attention

#### Description

Multi-head attention with relative positional encodings.

#### Usage

```r
rel_position_attention
```


## resample_audio

### Resample audio

#### Description

Resample audio

#### Usage

```r
resample_audio(samples, from_sr, to_sr)
```

#### Arguments

- **`samples`**: Numeric vector of audio samples
- **`from_sr`**: Source sample rate
- **`to_sr`**: Target sample rate

#### Value

Resampled audio samples


## rotate_half

### Rotate half of the tensor for RoPE

#### Description

Rotate half of the tensor for RoPE

#### Usage

```r
rotate_half(x)
```

#### Arguments

- **`x`**: Input tensor

#### Value

Rotated tensor


## s3_audio_encoder

### S3 Audio Encoder V2

#### Description

S3 Audio Encoder V2

#### Usage

```r
s3_audio_encoder
```


## s3_log_mel_spectrogram

### Compute log mel spectrogram for S3Tokenizer

#### Description

Compute log mel spectrogram for S3Tokenizer

#### Usage

```r
s3_log_mel_spectrogram(audio, mel_filters, window, n_fft = 400, device = "cpu")
```

#### Arguments

- **`audio`**: Audio tensor (batch, samples)
- **`mel_filters`**: Pre-computed mel filterbank
- **`window`**: Hann window
- **`n_fft`**: FFT size (default 400)
- **`device`**: Device

#### Value

Log mel spectrogram (batch, n_mels, time)


## s3_multi_head_attention

### Multi-Head Attention base module

#### Description

Multi-Head Attention base module

#### Usage

```r
s3_multi_head_attention
```


## s3_residual_attention_block

### Residual attention block

#### Description

Residual attention block

#### Usage

```r
s3_residual_attention_block
```


## s3_tokenizer_config

### S3Tokenizer model configuration

#### Description

S3Tokenizer model configuration

#### Usage

```r
s3_tokenizer_config(
  n_mels = 128,
  n_audio_state = 1280,
  n_audio_head = 20,
  n_audio_layer = 6,
  n_codebook_size = 6561
)
```

#### Arguments

- **`n_mels`**: Number of mel bins (default 128)
- **`n_audio_state`**: Hidden state dimension (default 1280)
- **`n_audio_head`**: Number of attention heads (default 20)
- **`n_audio_layer`**: Number of transformer layers (default 6)
- **`n_codebook_size`**: Codebook size (default 6561 = 3^8)

#### Value

Configuration list


## s3_tokenizer

### S3Tokenizer V2 module

#### Description

S3Tokenizer V2 module

#### Usage

```r
s3_tokenizer
```


## s3gen

### S3Gen Token to Waveform

#### Description

S3Gen Token to Waveform

#### Usage

```r
s3gen
```


## serve_chatterbox

### Serve Chatterbox as HTTP Microservice

#### Description

Convenience wrapper that starts a Chatterbox TTS server using
`gpu.ctl::serve()`. The model is loaded once at startup and
reused for all requests.

#### Usage

```r
serve_chatterbox(port = 7810L, host = "0.0.0.0", device = "cuda", traced = TRUE)
```

#### Arguments

- **`port`**: Integer. Port to listen on. Default 7810.
- **`host`**: Character. Bind address. Default "0.0.0.0".
- **`device`**: Device to use ("cpu" or "cuda"). Default "cuda".
- **`traced`**: Use JIT-traced inference. Default TRUE.

#### Details

Exposes the following endpoints:
- **GET /health**: Returns JSON with model status
- **POST /audio/speech**: Generate speech. JSON body with fields:
    `input` (text), `voice` (path to reference audio),
    `exaggeration` (0-1), `cfg_weight` (0-1),
    `temperature` (0-5). Returns raw WAV audio.

Requires the gpu.ctl package. Install with:
`install.packages("gpu.ctl")`

#### Value

Does not return (runs until interrupted).

#### Examples

```r
serve_chatterbox()
# curl -X POST http://localhost:7810/audio/speech \
#   -H "Content-Type: application/json" \
#   -d '{"input":"Hello world","voice":"/path/to/ref.wav"}' \
#   --output hello.wav
```


## sine_gen

### Sine Generator

#### Description

Generates sine waveforms from F0 for source-filter synthesis

#### Usage

```r
sine_gen
```


## sinusoidal_pos_emb

### Sinusoidal positional embedding for timesteps

#### Description

Sinusoidal positional embedding for timesteps

#### Usage

```r
sinusoidal_pos_emb
```


## snake_activation

### Snake activation function

#### Description

Sine-based periodic activation: x + 1/a * sin^2(ax)
Reference: https://arxiv.org/abs/2006.08195

#### Usage

```r
snake_activation
```


## source_module_hn_nsf

### Source Module for Neural Source Filter

#### Description

Source Module for Neural Source Filter

#### Usage

```r
source_module_hn_nsf
```


## statistics_pooling

### Statistics pooling

#### Description

Statistics pooling

#### Usage

```r
statistics_pooling(x)
```

#### Arguments

- **`x`**: Input tensor (batch, channels, time)

#### Value

Statistics tensor (batch, channels * 2)


## t3_cond_enc

### T3 conditioning encoder

#### Description

T3 conditioning encoder

#### Usage

```r
t3_cond_enc
```


## t3_cond_to_device

### Move T3 conditioning to device

#### Description

Move T3 conditioning to device

#### Usage

```r
t3_cond_to_device(cond, device)
```

#### Arguments

- **`cond`**: T3 conditioning object
- **`device`**: Target device

#### Value

T3 conditioning on device


## t3_cond

### Create T3 conditioning object

#### Description

Create T3 conditioning object

#### Usage

```r
t3_cond(
  speaker_emb,
  cond_prompt_speech_tokens = NULL,
  cond_prompt_speech_emb = NULL,
  emotion_adv = 0.5
)
```

#### Arguments

- **`speaker_emb`**: Speaker embedding tensor (B, 256)
- **`cond_prompt_speech_tokens`**: Optional speech tokens for conditioning
- **`cond_prompt_speech_emb`**: Optional pre-computed speech embeddings
- **`emotion_adv`**: Emotion/exaggeration control (0-1)

#### Value

List representing T3Cond


## t3_config_english

### Create T3 configuration (English-only)

#### Description

Create T3 configuration (English-only)

#### Usage

```r
t3_config_english()
```

#### Value

List with T3 configuration


## t3_inference_cpp

### T3 inference with C++ decode loop

#### Description

Runs prefill in R, then delegates the entire autoregressive decode loop
to C++ via a single .Call(). Eliminates R->lantern->libtorch per-op
dispatch overhead (~270 matmuls + ~100 elementwise ops per token).

#### Usage

```r
t3_inference_cpp(
  model,
  cond,
  text_tokens,
  max_new_tokens = 1000,
  temperature = 0.8,
  cfg_weight = 0.5,
  top_p = 0.95,
  min_p = 0.05,
  repetition_penalty = 1.2,
  max_cache_len = 350L
)
```

#### Arguments

- **`model`**: T3 model
- **`cond`**: T3 conditioning
- **`text_tokens`**: Tokenized text (tensor)
- **`max_new_tokens`**: Maximum speech tokens to generate
- **`temperature`**: Sampling temperature
- **`cfg_weight`**: Classifier-free guidance weight
- **`top_p`**: Nucleus sampling threshold
- **`min_p`**: Minimum probability threshold
- **`repetition_penalty`**: Repetition penalty
- **`max_cache_len`**: Maximum KV cache length

#### Value

Generated speech tokens (0-indexed integer vector)


## t3_inference_traced

### T3 inference with JIT tracing (optimized)

#### Description

Uses jit_trace for ~8x faster per-token inference.

#### Usage

```r
t3_inference_traced(
  model,
  cond,
  text_tokens,
  max_new_tokens = 1000,
  temperature = 0.8,
  cfg_weight = 0.5,
  top_p = 0.95,
  min_p = 0.05,
  repetition_penalty = 1.2,
  max_cache_len = 350L
)
```

#### Arguments

- **`model`**: T3 model
- **`cond`**: Conditioning object
- **`text_tokens`**: Text token tensor
- **`max_new_tokens`**: Maximum tokens to generate
- **`temperature`**: Sampling temperature
- **`cfg_weight`**: Classifier-free guidance weight
- **`top_p`**: Top-p sampling threshold
- **`min_p`**: Min-p filtering threshold
- **`repetition_penalty`**: Repetition penalty
- **`max_cache_len`**: Maximum KV cache length

#### Value

Generated speech token tensor


## t3_inference

### Run T3 inference to generate speech tokens

#### Description

Run T3 inference to generate speech tokens

#### Usage

```r
t3_inference(
  model,
  cond,
  text_tokens,
  max_new_tokens = 1000,
  temperature = 0.8,
  cfg_weight = 0.5,
  top_p = 0.95,
  min_p = 0.05,
  repetition_penalty = 1.2
)
```

#### Arguments

- **`model`**: T3 model
- **`cond`**: T3 conditioning
- **`text_tokens`**: Tokenized text (tensor)
- **`max_new_tokens`**: Maximum speech tokens to generate
- **`temperature`**: Sampling temperature
- **`cfg_weight`**: Classifier-free guidance weight
- **`top_p`**: Nucleus sampling threshold
- **`min_p`**: Minimum probability threshold
- **`repetition_penalty`**: Repetition penalty

#### Value

Generated speech tokens


## t3_model

### T3 Token-to-Token TTS model

#### Description

T3 Token-to-Token TTS model

#### Usage

```r
t3_model
```


## tdnn_layer

### TDNN Layer

#### Description

TDNN Layer

#### Usage

```r
tdnn_layer
```


## text_to_tokens

### Convert text to token tensor

#### Description

Convert text to token tensor

#### Usage

```r
text_to_tokens(tokenizer, text, normalize = TRUE, device = "cpu")
```

#### Arguments

- **`tokenizer`**: Tokenizer object
- **`text`**: Input text
- **`normalize`**: Whether to apply punctuation normalization
- **`device`**: Target device

#### Value

Token tensor (1, seq_len)


## timestep_embedding

### Timestep embedding MLP

#### Description

Timestep embedding MLP

#### Usage

```r
timestep_embedding
```


## tokenize_text

### Encode text to token IDs using BPE

#### Description

Implements proper BPE (Byte Pair Encoding) using the merge list.
Merges are applied in priority order (first merge = highest priority).

#### Usage

```r
tokenize_text(tokenizer, text)
```

#### Arguments

- **`tokenizer`**: Tokenizer object
- **`text`**: Input text

#### Value

Integer vector of token IDs


## traceable_attention

### Traceable attention module with pre-allocated KV cache

#### Description

This module is designed to be traced with jit_trace. It uses:
- Pre-allocated KV cache of fixed max size
- Attention mask to indicate valid cache positions
- Returns only output tensor (no lists/dicts)

#### Usage

```r
traceable_attention
```


## traceable_decoder_layer

### Traceable decoder layer with pre-allocated KV cache

#### Description

Traceable decoder layer with pre-allocated KV cache

#### Usage

```r
traceable_decoder_layer
```


## traceable_kv_projector

### Traceable K/V projection module

#### Description

Computes K and V projections with RoPE for a single layer.
Returns concatenated K and V for easy unpacking.

#### Usage

```r
traceable_kv_projector
```


## traceable_transformer_cached

### Traceable transformer for cached inference

#### Description

This wraps the full Llama model for traced cached inference.
Uses pre-allocated KV cache for all layers.

#### Usage

```r
traceable_transformer_cached
```


## traceable_transformer_first

### Traceable transformer for first token (no cache)

#### Description

Traceable transformer for first token (no cache)

#### Usage

```r
traceable_transformer_first
```


## transit_layer

### Transit layer (channel reduction)

#### Description

Transit layer (channel reduction)

#### Usage

```r
transit_layer
```


## transpose_layer

### Transpose layer for use in sequential

#### Description

Transpose layer for use in sequential

#### Usage

```r
transpose_layer
```


## tts_chunked

### Generate speech in chunks (for long texts)

#### Description

Generate speech in chunks (for long texts)

#### Usage

```r
tts_chunked(model, text, voice, chunk_size = 200, ...)
```

#### Arguments

- **`model`**: Chatterbox model
- **`text`**: Text to synthesize
- **`voice`**: Voice embedding
- **`chunk_size`**: Maximum tokens per chunk
- **`...`**: Additional arguments passed to generate()

#### Value

List with audio and sample_rate


## tts_to_file

### Generate speech and save to file

#### Description

Generate speech and save to file

#### Usage

```r
tts_to_file(model, text, voice, output_path, ...)
```

#### Arguments

- **`model`**: Chatterbox model
- **`text`**: Text to synthesize
- **`voice`**: Voice embedding or path to reference audio
- **`output_path`**: Output file path (WAV format)
- **`...`**: Additional arguments passed to generate()

#### Value

Invisibly returns the output path


## tts

### Generate speech from text

#### Description

Generate speech from text

#### Usage

```r
generate(
  model,
  text,
  voice,
  exaggeration = 0.5,
  cfg_weight = 0.5,
  temperature = 0.8,
  top_p = 0.9,
  autocast = NULL,
  traced = FALSE
)
```

#### Arguments

- **`model`**: Chatterbox model
- **`text`**: Text to synthesize
- **`voice`**: Voice embedding from create_voice_embedding() or path to reference audio
- **`exaggeration`**: Emotion/expression exaggeration level (0-1, default 0.5)
- **`cfg_weight`**: Classifier-free guidance weight (higher = more adherence to text, default 0.5)
- **`temperature`**: Sampling temperature (default 0.8)
- **`top_p`**: Top-p (nucleus) sampling threshold (default 0.9)
- **`autocast`**: Use mixed precision (float16) on CUDA for faster inference (default TRUE on CUDA)

#### Value

List with audio (numeric vector) and sample_rate


## turbo_models_available

### Check if Turbo Models are Downloaded

#### Description

Check if Turbo Models are Downloaded

#### Usage

```r
turbo_models_available()
```

#### Value

TRUE if all turbo model files exist locally

#### Examples

```r
turbo_models_available()
```


## update_kv_cache

### Update KV cache with new K/V values

#### Description

Update KV cache with new K/V values

#### Usage

```r
update_kv_cache(cache, layer_idx, new_k, new_v, position)
```

#### Arguments

- **`cache`**: Cache list from create_kv_cache
- **`layer_idx`**: Layer index (1-indexed)
- **`new_k`**: New key tensor (batch, heads, 1, head_dim)
- **`new_v`**: New value tensor (batch, heads, 1, head_dim)
- **`position`**: Current position (0-indexed)


## update_valid_mask

### Update valid mask to include new position

#### Description

Update valid mask to include new position

#### Usage

```r
update_valid_mask(cache, position)
```

#### Arguments

- **`cache`**: Cache list from create_kv_cache
- **`position`**: Current position (0-indexed)


## upsample_1d

### Upsample 1D

#### Description

2x upsampling using interpolation + convolution.

#### Usage

```r
upsample_1d
```


## upsample_conformer_encoder_full

### Upsample Conformer Encoder

#### Description

Full conformer encoder matching Python UpsampleConformerEncoder.

#### Usage

```r
upsample_conformer_encoder_full
```


## upsample_conformer_encoder

### Upsample Conformer Encoder

#### Description

Upsample Conformer Encoder

#### Usage

```r
upsample_conformer_encoder(input_size = 512, output_size = 512, num_blocks = 6)
```

#### Arguments

- **`input_size`**: Input dimension
- **`output_size`**: Output dimension
- **`num_blocks`**: Number of conformer blocks

#### Value

nn_module


## voice_encoder_config

### Voice encoder configuration

#### Description

Voice encoder configuration

#### Usage

```r
voice_encoder_config()
```

#### Value

List with configuration parameters


## voice_encoder

### Voice encoder module

#### Description

Voice encoder module

#### Usage

```r
voice_encoder
```


## write_audio

### Write audio file

#### Description

Write audio file

#### Usage

```r
write_audio(samples, sr, path)
```

#### Arguments

- **`samples`**: Numeric vector of audio samples (normalized to \[-1, 1\])
- **`sr`**: Sample rate
- **`path`**: Output path (WAV format)


